{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oRXk5DU9BYc"
      },
      "source": [
        "# Image classification with ConvMixer\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/10/12<br>\n",
        "**Last modified:** 2021/10/12<br>\n",
        "**Description:** An all-convolutional network applied to patches of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiO76Iov9BYi"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Vision Transformers (ViT; [Dosovitskiy et al.](https://arxiv.org/abs/1612.00593)) extract\n",
        "small patches from the input images, linearly project them, and then apply the\n",
        "Transformer ([Vaswani et al.](https://arxiv.org/abs/1706.03762)) blocks. The application\n",
        "of ViTs to image recognition tasks is quickly becoming a promising area of research,\n",
        "because ViTs eliminate the need to have strong inductive biases (such as convolutions) for\n",
        "modeling locality. This presents them as a general computation primititive capable of\n",
        "learning just from the training data with as minimal inductive priors as possible. ViTs\n",
        "yield great downstream performance when trained with proper regularization, data\n",
        "augmentation, and relatively large datasets.\n",
        "\n",
        "In the [Patches Are All You Need](https://openreview.net/pdf?id=TVHS5Y4dNvM) paper (note: at\n",
        "the time of writing, it is a submission to the ICLR 2022 conference), the authors extend\n",
        "the idea of using patches to train an all-convolutional network and demonstrate\n",
        "competitive results. Their architecture namely **ConvMixer** uses recipes from the recent\n",
        "isotrophic architectures like ViT, MLP-Mixer\n",
        "([Tolstikhin et al.](https://arxiv.org/abs/2105.01601)), such as using the same\n",
        "depth and resolution across different layers in the network, residual connections,\n",
        "and so on.\n",
        "\n",
        "In this example, we will implement the ConvMixer model and demonstrate its performance on\n",
        "the CIFAR-10 dataset.\n",
        "\n",
        "To use the AdamW optimizer, we need to install TensorFlow Addons:\n",
        "\n",
        "```shell\n",
        "pip install -U -q tensorflow-addons\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U -q tensorflow-addons"
      ],
      "metadata": {
        "id": "4z1Y35oS9vw5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CXbe1Wd79y9t",
        "outputId": "7c231d45-8f2e-4afc-81f7-51514f20b951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIa4pA7B9BYj"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LCOaIkLu9BYk",
        "outputId": "594f7504-6ac1-4cd7-e095-d898bc3f7999",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUoQxHnD9BYl"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "To keep run time short, we will train the model for only 10 epochs. To focus on\n",
        "the core ideas of ConvMixer, we will not use other training-specific elements like\n",
        "RandAugment ([Cubuk et al.](https://arxiv.org/abs/1909.13719)). If you are interested in\n",
        "learning more about those details, please refer to the\n",
        "[original paper](https://openreview.net/pdf?id=TVHS5Y4dNvM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GE70RomR9BYm"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2,os\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "4-grBh1x97T4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(folder_path))\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(folder_path, class_name)\n",
        "        for filename in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (128,128))  # resize images to 32x32\n",
        "            images.append(img)\n",
        "            labels.append(class_name)\n",
        "    label_to_index = dict((name, index) for index, name in enumerate(class_names))\n",
        "    labels = [label_to_index[label] for label in labels]\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n"
      ],
      "metadata": {
        "id": "oBpivRJQ9qpQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = load_images_from_folder('/content/drive/MyDrive/Azymer/AlzimerSVMModel/PreprocessSeg')\n",
        "num_classes = len(set(labels))\n",
        "labels = to_categorical(labels, 4)\n"
      ],
      "metadata": {
        "id": "PE8ychgz9qsw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_ratio = 0.8\n",
        "split_index = int(split_ratio * len(data))\n",
        "train_data, train_labels = data[:split_index], labels[:split_index]\n",
        "test_data, test_labels = data[split_index:], labels[split_index:]\n"
      ],
      "metadata": {
        "id": "92plMC7Z9qx0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDpiCaV39BYn"
      },
      "source": [
        "## Load the CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rX-35Wst9BYo",
        "outputId": "61b90f3e-bed7-4a64-9a48-13f1e2511ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data samples: 4608\n",
            "Validation data samples: 512\n"
          ]
        }
      ],
      "source": [
        "val_split = 0.1\n",
        "\n",
        "val_indices = int(len(train_data) * val_split)\n",
        "new_x_train, new_y_train = train_data[val_indices:], train_labels[val_indices:]\n",
        "x_val, y_val = train_data[:val_indices], train_labels[:val_indices]\n",
        "new_x_test, new_y_test = test_data[val_indices:], test_labels[val_indices:]\n",
        "x_test, y_test = test_data[:val_indices], test_labels[:val_indices]\n",
        "\n",
        "\n",
        "print(f\"Training data samples: {len(new_x_train)}\")\n",
        "print(f\"Validation data samples: {len(x_val)}\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define your model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels))\n"
      ],
      "metadata": {
        "id": "hSlS7sVQHcvZ",
        "outputId": "bea947a3-7ee9-4782-a5fa-d1de3896d9bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "160/160 [==============================] - 195s 1s/step - loss: 7.5134 - accuracy: 0.6178 - val_loss: 1.6361 - val_accuracy: 0.0281\n",
            "Epoch 2/10\n",
            "160/160 [==============================] - 205s 1s/step - loss: 0.7588 - accuracy: 0.6855 - val_loss: 1.4982 - val_accuracy: 0.1602\n",
            "Epoch 3/10\n",
            "160/160 [==============================] - 200s 1s/step - loss: 0.6857 - accuracy: 0.7229 - val_loss: 1.2928 - val_accuracy: 0.3297\n",
            "Epoch 4/10\n",
            "160/160 [==============================] - 198s 1s/step - loss: 0.6110 - accuracy: 0.7598 - val_loss: 1.9251 - val_accuracy: 0.1227\n",
            "Epoch 5/10\n",
            "160/160 [==============================] - 213s 1s/step - loss: 0.5411 - accuracy: 0.7863 - val_loss: 2.0097 - val_accuracy: 0.1789\n",
            "Epoch 6/10\n",
            "160/160 [==============================] - 201s 1s/step - loss: 0.4742 - accuracy: 0.8154 - val_loss: 1.1552 - val_accuracy: 0.4563\n",
            "Epoch 7/10\n",
            "160/160 [==============================] - 200s 1s/step - loss: 0.4060 - accuracy: 0.8434 - val_loss: 0.9868 - val_accuracy: 0.5586\n",
            "Epoch 8/10\n",
            "160/160 [==============================] - 198s 1s/step - loss: 0.3311 - accuracy: 0.8719 - val_loss: 1.1276 - val_accuracy: 0.5023\n",
            "Epoch 9/10\n",
            "160/160 [==============================] - 197s 1s/step - loss: 0.2821 - accuracy: 0.8902 - val_loss: 1.5628 - val_accuracy: 0.4000\n",
            "Epoch 10/10\n",
            "160/160 [==============================] - 195s 1s/step - loss: 0.2692 - accuracy: 0.8965 - val_loss: 1.3806 - val_accuracy: 0.4664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae1867ec70>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained model\n",
        "model = keras.applications.ResNet50(weights='imagenet')\n",
        "\n",
        "# Load and preprocess image\n",
        "img_path = '/content/drive/MyDrive/Azymer/AlzimerSVMModel/Preprocessd/Mild_Demented/0.png'\n",
        "img = keras.preprocessing.image.load_img(img_path, target_size=(224,224))\n",
        "x = keras.preprocessing.image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "# Make prediction\n",
        "preds = model.predict(x)\n",
        "\n",
        "# Decode the predictions and display top 5 classes\n",
        "decoded_preds = decode_predictions(preds, top=4)[0]\n",
        "for pred in decoded_preds:\n",
        "    print(pred[1], ':', pred[2])\n"
      ],
      "metadata": {
        "id": "eoRNT8LXqFAv",
        "outputId": "892716ea-9ea0-4593-b3f1-4b67a9539758",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n",
            "stopwatch : 0.1569954\n",
            "pick : 0.119139\n",
            "chambered_nautilus : 0.060526945\n",
            "shield : 0.05736747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "Ux8H1jj6w2HQ",
        "outputId": "66cc0658-837c-4a3d-892b-15fb1d4bed14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-21c3b0e102b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3683\u001b[0m         \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3685\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   3686\u001b[0m                 \u001b[0;34m\"You must compile your model before \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m                 \u001b[0;34m\"training/testing. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (128,128,3)\n",
        "\n",
        "# Define the ConvMixer architecture\n",
        "def convmixer_model(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid')(inputs)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Permute((3, 1, 2))(x)\n",
        "    x = layers.Reshape((-1, x.shape[-1]))(x)\n",
        "    num_patches = x.shape[1]\n",
        "    x = layers.Dense(units=256, activation='gelu')(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    x = layers.Dense(units=512)(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Dense(units=num_classes, activation='softmax')(x)\n",
        "    outputs = layers.Flatten()(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Create the model\n",
        "model = convmixer_model(input_shape, num_classes=4)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_data, train_labels, \n",
        "    batch_size=32, \n",
        "    epochs=10, \n",
        "    validation_data=(x_val, y_val)\n",
        ")\n"
      ],
      "metadata": {
        "id": "TZbcV2hXIgQM",
        "outputId": "9827c1fe-cd9f-4b47-bc91-7f61a21abbd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f64df7749f79>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 2156, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.9/dist-packages/keras/backend.py\", line 5715, in binary_crossentropy\n        bce = target * tf.math.log(output + epsilon())\n\n    ValueError: Dimensions must be equal, but are 4 and 122880 for '{{node binary_crossentropy/mul}} = Mul[T=DT_FLOAT](IteratorGetNext:1, binary_crossentropy/Log)' with input shapes: [32,4], [32,122880].\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "convmixer",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}