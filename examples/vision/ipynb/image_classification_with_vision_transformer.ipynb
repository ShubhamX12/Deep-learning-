{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ6ZaCjEIrMf"
      },
      "source": [
        "# Image classification with Vision Transformer\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2021/01/18<br>\n",
        "**Last modified:** 2021/01/18<br>\n",
        "**Description:** Implementing the Vision Transformer (ViT) model for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1PFJU1sIrMm"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n",
        "model by Alexey Dosovitskiy et al. for image classification,\n",
        "and demonstrates it on the CIFAR-100 dataset.\n",
        "The ViT model applies the Transformer architecture with self-attention to sequences of\n",
        "image patches, without using convolution layers.\n",
        "\n",
        "This example requires TensorFlow 2.4 or higher, as well as\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n",
        "which can be installed using the following command:\n",
        "\n",
        "```python\n",
        "pip install -U tensorflow-addons\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "zFO5uYx2IyJj",
        "outputId": "a35ded19-a32a-4e1a-9d16-52a3722a1f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Using cached tensorflow_addons-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.1)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKwK16gbIrMn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tAt8pkMaIrMo",
        "outputId": "0f3f7c8e-4513-4b8b-e1c5-6f51498cee30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z0TkKAHjJ2qn",
        "outputId": "675e5766-be56-4438-d9fc-92fcbc3ea7c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS96bCi5IrMq"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L2zc9worIrMr",
        "outputId": "ba117612-19ba-44e4-bd31-f23ea0d7a48c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17822 images belonging to 4 classes.\n",
            "Found 1277 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# define data generators for training and testing data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# load image data from folders\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/AlzheimeDataset/train',\n",
        "        target_size=(128,128),\n",
        "        batch_size=128,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/AlzheimeDataset/test',\n",
        "        target_size=(128, 128),\n",
        "        batch_size=128,\n",
        "        class_mode='binary')\n",
        "\n",
        "# split data into separate variables\n",
        "x_train, y_train = train_generator.next()\n",
        "x_test, y_test = test_generator.next()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.squeeze()\n",
        "y_test = y_test.squeeze()"
      ],
      "metadata": {
        "id": "3AsR4OdRPqYn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "input_shape = (128, 128, 3)\n",
        " \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "ZDtvDxasJz5m",
        "outputId": "b91829b3-98f1-4023-ece1-2cedfa767de5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (128, 128, 128, 3) - y_train shape: (128, 4, 4)\n",
            "x_test shape: (128, 128, 128, 3) - y_test shape: (128, 4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx0xH-g6IrMs"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DG5Lb4nKIrMt"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXybv149IrMu"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ej6Knw7qIrMv"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEaldOYqIrMw"
      },
      "source": [
        "## Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ghd3JBOvIrMw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEPZ2TtbIrMx"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rkA6cDSnIrMy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRaefIZCIrMz"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "z0ZHt2EpIrMz",
        "outputId": "0420e6dc-9bbe-415e-e568-5ac719ae593d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 144\n",
            "Elements per patch: 108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFICAYAAAAyFGczAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEOElEQVR4nO3bsQ2EQAwAQQ7Rf8umALTSJfwTzMQOHK2ceM3MHAA8nP9eAOCrBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEK7dwbXWm3sA/MzuA6ELEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgCCQAEEgAYJAAgSBBAgCCRAEEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIgCCRAEEiAIJAAQSABgkACBIEECAIJEAQSIAgkQBBIgHDtDs7Mm3sAfI4LEiAIJEAQSIAgkABBIAGCQAIEgQQIAgkQBBIg3AA6DI3YutA5AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 144 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt1TogQLIrM0"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
        "vector of size `projection_dim`. In addition, it adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZLOn1TJxIrM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V-mAXyuIrM1"
      },
      "source": [
        "## Build the ViT model\n",
        "\n",
        "The ViT model consists of multiple Transformer blocks,\n",
        "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
        "applied to the sequence of patches. The Transformer blocks produce a\n",
        "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
        "classifier head with softmax to produce the final class probabilities output.\n",
        "\n",
        "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
        "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
        "as the image representation, all the outputs of the final Transformer block are\n",
        "reshaped with `layers.Flatten()` and used as the image\n",
        "representation input to the classifier head.\n",
        "Note that the `layers.GlobalAveragePooling1D` layer\n",
        "could also be used instead to aggregate the outputs of the Transformer block,\n",
        "especially when the number of patches and the projection dimensions are large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3lNlPBLiIrM2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDvJdMGaIrM3"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Bkb_T9ucIrM4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy , 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ],
      "metadata": {
        "id": "HBu1WLUROdxD",
        "outputId": "ffd26ed4-599c-4949-a5eb-3a48e49d8dc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 30s 30s/step - loss: 4.1432 - accuracy: 0.1391 - top-5-accuracy: 1.0000 - val_loss: 11.4658 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 12s 12s/step - loss: 5.2652 - accuracy: 0.4348 - top-5-accuracy: 1.0000 - val_loss: 37.0129 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 10s 10s/step - loss: 20.1436 - accuracy: 0.1826 - top-5-accuracy: 1.0000 - val_loss: 25.7284 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 16.3217 - accuracy: 0.3913 - top-5-accuracy: 1.0000 - val_loss: 18.4956 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 10s 10s/step - loss: 14.3423 - accuracy: 0.4174 - top-5-accuracy: 1.0000 - val_loss: 3.9307 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 3.5312 - accuracy: 0.4348 - top-5-accuracy: 1.0000 - val_loss: 14.8583 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 9.5756 - accuracy: 0.2087 - top-5-accuracy: 1.0000 - val_loss: 8.0390 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 6.4516 - accuracy: 0.4261 - top-5-accuracy: 1.0000 - val_loss: 5.8256 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 4.5155 - accuracy: 0.4348 - top-5-accuracy: 1.0000 - val_loss: 4.2511 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 3.7788 - accuracy: 0.4870 - top-5-accuracy: 1.0000 - val_loss: 4.0656 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 3.9400 - accuracy: 0.4174 - top-5-accuracy: 1.0000 - val_loss: 3.3909 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 2.5836 - accuracy: 0.5043 - top-5-accuracy: 1.0000 - val_loss: 2.5662 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 2.0703 - accuracy: 0.4348 - top-5-accuracy: 1.0000 - val_loss: 2.6313 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.7268 - accuracy: 0.3478 - top-5-accuracy: 1.0000 - val_loss: 2.8279 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.7521 - accuracy: 0.3478 - top-5-accuracy: 1.0000 - val_loss: 2.4952 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.6006 - accuracy: 0.4000 - top-5-accuracy: 1.0000 - val_loss: 1.8914 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.3818 - accuracy: 0.3478 - top-5-accuracy: 1.0000 - val_loss: 1.6812 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.4310 - accuracy: 0.4087 - top-5-accuracy: 1.0000 - val_loss: 1.6845 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.3371 - accuracy: 0.4957 - top-5-accuracy: 1.0000 - val_loss: 1.5876 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.4397 - accuracy: 0.4348 - top-5-accuracy: 1.0000 - val_loss: 1.4903 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.3004 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.5111 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.3273 - accuracy: 0.5043 - top-5-accuracy: 1.0000 - val_loss: 1.5617 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.2169 - accuracy: 0.4522 - top-5-accuracy: 1.0000 - val_loss: 1.6075 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1417 - accuracy: 0.5130 - top-5-accuracy: 1.0000 - val_loss: 1.6334 - val_accuracy: 0.2308 - val_top-5-accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.3762 - accuracy: 0.3739 - top-5-accuracy: 1.0000 - val_loss: 1.6158 - val_accuracy: 0.3077 - val_top-5-accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.2109 - accuracy: 0.4087 - top-5-accuracy: 1.0000 - val_loss: 1.5714 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.2661 - accuracy: 0.4435 - top-5-accuracy: 1.0000 - val_loss: 1.5621 - val_accuracy: 0.2308 - val_top-5-accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.2857 - accuracy: 0.4435 - top-5-accuracy: 1.0000 - val_loss: 1.5935 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.2805 - accuracy: 0.4435 - top-5-accuracy: 1.0000 - val_loss: 1.5717 - val_accuracy: 0.3077 - val_top-5-accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.1131 - accuracy: 0.4783 - top-5-accuracy: 1.0000 - val_loss: 1.5698 - val_accuracy: 0.2308 - val_top-5-accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1267 - accuracy: 0.4174 - top-5-accuracy: 1.0000 - val_loss: 1.5780 - val_accuracy: 0.1538 - val_top-5-accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.2916 - accuracy: 0.4087 - top-5-accuracy: 1.0000 - val_loss: 1.5502 - val_accuracy: 0.3077 - val_top-5-accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1654 - accuracy: 0.4522 - top-5-accuracy: 1.0000 - val_loss: 1.4774 - val_accuracy: 0.2308 - val_top-5-accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 10s 10s/step - loss: 1.0358 - accuracy: 0.5217 - top-5-accuracy: 1.0000 - val_loss: 1.4153 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.2324 - accuracy: 0.4435 - top-5-accuracy: 1.0000 - val_loss: 1.3925 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1325 - accuracy: 0.5130 - top-5-accuracy: 1.0000 - val_loss: 1.4002 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.1303 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.4058 - val_accuracy: 0.3077 - val_top-5-accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1021 - accuracy: 0.4870 - top-5-accuracy: 1.0000 - val_loss: 1.4111 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1539 - accuracy: 0.4696 - top-5-accuracy: 1.0000 - val_loss: 1.3946 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0512 - accuracy: 0.5478 - top-5-accuracy: 1.0000 - val_loss: 1.3866 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1152 - accuracy: 0.4348 - top-5-accuracy: 1.0000 - val_loss: 1.3727 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.1624 - accuracy: 0.3739 - top-5-accuracy: 1.0000 - val_loss: 1.3594 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0847 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.3593 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.1567 - accuracy: 0.4174 - top-5-accuracy: 1.0000 - val_loss: 1.3532 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0757 - accuracy: 0.5130 - top-5-accuracy: 1.0000 - val_loss: 1.3294 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.9478 - accuracy: 0.5565 - top-5-accuracy: 1.0000 - val_loss: 1.3044 - val_accuracy: 0.2308 - val_top-5-accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.0940 - accuracy: 0.5304 - top-5-accuracy: 1.0000 - val_loss: 1.2861 - val_accuracy: 0.3077 - val_top-5-accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0003 - accuracy: 0.4957 - top-5-accuracy: 1.0000 - val_loss: 1.2890 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.1111 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.2839 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1019 - accuracy: 0.4522 - top-5-accuracy: 1.0000 - val_loss: 1.2736 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1380 - accuracy: 0.4261 - top-5-accuracy: 1.0000 - val_loss: 1.2879 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0789 - accuracy: 0.5043 - top-5-accuracy: 1.0000 - val_loss: 1.3038 - val_accuracy: 0.3077 - val_top-5-accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0802 - accuracy: 0.4696 - top-5-accuracy: 1.0000 - val_loss: 1.3100 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.1366 - accuracy: 0.4000 - top-5-accuracy: 1.0000 - val_loss: 1.3084 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1529 - accuracy: 0.4435 - top-5-accuracy: 1.0000 - val_loss: 1.3073 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0886 - accuracy: 0.4783 - top-5-accuracy: 1.0000 - val_loss: 1.3009 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0985 - accuracy: 0.4870 - top-5-accuracy: 1.0000 - val_loss: 1.2932 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0714 - accuracy: 0.4870 - top-5-accuracy: 1.0000 - val_loss: 1.2840 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.1724 - accuracy: 0.3913 - top-5-accuracy: 1.0000 - val_loss: 1.2883 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0955 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.2828 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.0378 - accuracy: 0.4783 - top-5-accuracy: 1.0000 - val_loss: 1.2840 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0340 - accuracy: 0.4957 - top-5-accuracy: 1.0000 - val_loss: 1.2913 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0414 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.2849 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.1197 - accuracy: 0.4957 - top-5-accuracy: 1.0000 - val_loss: 1.2790 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0582 - accuracy: 0.5043 - top-5-accuracy: 1.0000 - val_loss: 1.2630 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.0062 - accuracy: 0.5130 - top-5-accuracy: 1.0000 - val_loss: 1.2552 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0827 - accuracy: 0.4522 - top-5-accuracy: 1.0000 - val_loss: 1.2565 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0573 - accuracy: 0.4696 - top-5-accuracy: 1.0000 - val_loss: 1.2575 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0353 - accuracy: 0.5130 - top-5-accuracy: 1.0000 - val_loss: 1.2504 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1205 - accuracy: 0.4522 - top-5-accuracy: 1.0000 - val_loss: 1.2460 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0124 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.2414 - val_accuracy: 0.6923 - val_top-5-accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 13s 13s/step - loss: 1.1336 - accuracy: 0.4783 - top-5-accuracy: 1.0000 - val_loss: 1.2332 - val_accuracy: 0.6154 - val_top-5-accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 12s 12s/step - loss: 0.9640 - accuracy: 0.5304 - top-5-accuracy: 1.0000 - val_loss: 1.2440 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.0591 - accuracy: 0.4696 - top-5-accuracy: 1.0000 - val_loss: 1.2604 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.1336 - accuracy: 0.4087 - top-5-accuracy: 1.0000 - val_loss: 1.2688 - val_accuracy: 0.6154 - val_top-5-accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0709 - accuracy: 0.4870 - top-5-accuracy: 1.0000 - val_loss: 1.2759 - val_accuracy: 0.6154 - val_top-5-accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0523 - accuracy: 0.5130 - top-5-accuracy: 1.0000 - val_loss: 1.2938 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.9868 - accuracy: 0.5565 - top-5-accuracy: 1.0000 - val_loss: 1.2969 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.9779 - accuracy: 0.5304 - top-5-accuracy: 1.0000 - val_loss: 1.2897 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.8939 - accuracy: 0.6087 - top-5-accuracy: 1.0000 - val_loss: 1.2723 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.9647 - accuracy: 0.5304 - top-5-accuracy: 1.0000 - val_loss: 1.2638 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0745 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.2325 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.9859 - accuracy: 0.4783 - top-5-accuracy: 1.0000 - val_loss: 1.2037 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 1.0316 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.1733 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.9607 - accuracy: 0.5391 - top-5-accuracy: 1.0000 - val_loss: 1.1771 - val_accuracy: 0.6154 - val_top-5-accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.9666 - accuracy: 0.5217 - top-5-accuracy: 1.0000 - val_loss: 1.1895 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.9944 - accuracy: 0.4783 - top-5-accuracy: 1.0000 - val_loss: 1.2097 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.9102 - accuracy: 0.6000 - top-5-accuracy: 1.0000 - val_loss: 1.2502 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0157 - accuracy: 0.4870 - top-5-accuracy: 1.0000 - val_loss: 1.2775 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.9914 - accuracy: 0.5913 - top-5-accuracy: 1.0000 - val_loss: 1.2940 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.0151 - accuracy: 0.4870 - top-5-accuracy: 1.0000 - val_loss: 1.2807 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0111 - accuracy: 0.4261 - top-5-accuracy: 1.0000 - val_loss: 1.2589 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.9913 - accuracy: 0.5043 - top-5-accuracy: 1.0000 - val_loss: 1.2146 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.9994 - accuracy: 0.4696 - top-5-accuracy: 1.0000 - val_loss: 1.1913 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.9628 - accuracy: 0.4783 - top-5-accuracy: 1.0000 - val_loss: 1.1774 - val_accuracy: 0.5385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.9503 - accuracy: 0.5652 - top-5-accuracy: 1.0000 - val_loss: 1.1717 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 1.0100 - accuracy: 0.5391 - top-5-accuracy: 1.0000 - val_loss: 1.1682 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.9884 - accuracy: 0.4609 - top-5-accuracy: 1.0000 - val_loss: 1.1587 - val_accuracy: 0.4615 - val_top-5-accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.8920 - accuracy: 0.5304 - top-5-accuracy: 1.0000 - val_loss: 1.1699 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.9669 - accuracy: 0.5043 - top-5-accuracy: 1.0000 - val_loss: 1.1693 - val_accuracy: 0.3846 - val_top-5-accuracy: 1.0000\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.0658 - accuracy: 0.4219 - top-5-accuracy: 1.0000\n",
            "Test accuracy: 42.19%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFSpQLLsIrM5"
      },
      "source": [
        "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
        "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
        "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
        "\n",
        "Note that the state of the art results reported in the\n",
        "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
        "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
        "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
        "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
        "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
        "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
        "In practice, it's recommended to fine-tune a ViT model\n",
        "that was pre-trained using a large, high-resolution dataset."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_classification_with_vision_transformer",
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}