{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Convmixer Image Classfication\n"
      ],
      "metadata": {
        "id": "UTXuzSKBDvAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U -q tensorflow-addons"
      ],
      "metadata": {
        "id": "BTo-5q1eDsY0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uME3dCbWxed0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W-jSeUEDxed5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N6cx2lbHxeew"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GL6njKb2zo90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Bmx-0GxSzqNA",
        "outputId": "69e43d19-bee5-4be8-d24f-09944b52d448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/PreprocessSeg/\"\n",
        "\n",
        "# Initialize empty lists to store the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Loop through all the subfolders and their files\n",
        "for subdir, _, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        # Load the image using OpenCV\n",
        "        image = cv2.imread(os.path.join(subdir, file))\n",
        "        # Resize the image to the desired size\n",
        "        image = cv2.resize(image, (64, 64))\n",
        "        # Normalize the image pixel values to be between 0 and 1\n",
        "        image = image / 255.0\n",
        "        # Append the image to the list of images\n",
        "        images.append(image)\n",
        "        # Extract the label from the folder name\n",
        "        label = os.path.basename(subdir)\n",
        "        # Append the label to the list of labels\n",
        "        labels.append(label)\n",
        "\n",
        "# Convert the lists of images and labels to numpy arrays\n",
        "images = np.array(images)\n",
        "num_classes = 4\n",
        "label_map = {'Mild_Demented': 0, 'Moderate_Demented': 1, 'Non_Demented': 2, 'Very_Mild_Demented': 3}\n",
        "labels_int = np.array([label_map[label] for label in labels])\n",
        "labels_one_hot = np.eye(num_classes)[labels_int]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "val_split = 0.1\n",
        "test_split = 0.1\n",
        "num_val = int(len(images) * val_split)\n",
        "num_test = int(len(images) * test_split)\n",
        "num_train = len(images) - num_val - num_test\n",
        "\n",
        "x_train, y_train = images[:num_train], labels_one_hot[:num_train]\n",
        "x_val, y_val = images[num_train:num_train+num_val], labels_one_hot[num_train:num_train+num_val] \n",
        "\n",
        "len(x_train)"
      ],
      "metadata": {
        "id": "-GhlxH4Hzk-T",
        "outputId": "2632c34d-5eec-4c37-9fbb-7dc6fabcea5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1537"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "test_data = test_data_generator.flow_from_directory(directory=\"/content/drive/MyDrive/PreprocessSeg/\",\n",
        "                                                    target_size=(128,128),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=False)"
      ],
      "metadata": {
        "id": "ReP2ciSckvAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f900ff-bc67-450f-bdfd-a1054baf5377"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1921 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0SQdtTY8xegV",
        "outputId": "a0b5c8a5-6893-4281-95f2-39f015dbaabb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data samples: 1384\n",
            "Validation data samples: 153\n"
          ]
        }
      ],
      "source": [
        "#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "val_split = 0.1\n",
        "\n",
        "val_indices = int(len(x_train) * val_split)\n",
        "new_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:]\n",
        "x_val, y_val = x_train[:val_indices], y_train[:val_indices]\n",
        "\n",
        "print(f\"Training data samples: {len(new_x_train)}\")\n",
        "print(f\"Validation data samples: {len(x_val)}\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assume y_train and y_test have a shape of [?, 4]\n",
        "# Convert them to integer labels\n",
        "y_train_int = np.argmax(y_train, axis=1)\n",
        "y_test_int = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Now you can convert them to one-hot encoded vectors with num_classes=4\n",
        "y_train = tf.keras.utils.to_categorical(y_train_int, num_classes=4)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_int, num_classes=4)\n",
        "\n",
        "y_train = y_train.squeeze()\n",
        "y_test = y_test.squeeze()\n"
      ],
      "metadata": {
        "id": "W6Br9_MSkIOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "82bf1d82-0f57-496b-c197-992987fdace9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f252d37798b1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Convert them to integer labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_train_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_test_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Now you can convert them to one-hot encoded vectors with num_classes=4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdwVUtpXxegn"
      },
      "source": [
        "## ConvMixer utilities\n",
        "\n",
        "The following figure (taken from the original paper) depicts the ConvMixer model:\n",
        "\n",
        "![](https://i.imgur.com/yF8actg.png)\n",
        "\n",
        "ConvMixer is very similar to the MLP-Mixer, model with the following key\n",
        "differences:\n",
        "\n",
        "* Instead of using fully-connected layers, it uses standard convolution layers.\n",
        "* Instead of LayerNorm (which is typical for ViTs and MLP-Mixers), it uses BatchNorm.\n",
        "\n",
        "Two types of convolution layers are used in ConvMixer. **(1)**: Depthwise convolutions,\n",
        "for mixing spatial locations of the images, **(2)**: Pointwise convolutions (which follow\n",
        "the depthwise convolutions), for mixing channel-wise information across the patches.\n",
        "Another keypoint is the use of *larger kernel sizes* to allow a larger receptive field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YPE2IxmbxehD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def activation_block(x):\n",
        "    x = layers.Activation(\"gelu\")(x)\n",
        "    return layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int):\n",
        "    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "    return activation_block(x)\n",
        "\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = layers.Add()([activation_block(x), x0])  # Residual.\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = activation_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_conv_mixer_256_8(\n",
        "    image_size=64, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=4\n",
        "):\n",
        "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
        "    The hyperparameter values are taken from the paper.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size)\n",
        "\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size)\n",
        "\n",
        "    # Classification block.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n"
      ],
      "metadata": {
        "id": "Wi8fegomA4ed"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKISV-yrxehS"
      },
      "source": [
        "## Model training and evaluation utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gq1Box8BxehT"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\", \n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, \n",
        "        y_train,  \n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model.evaluate(test_data)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu3Lk-zWxehW"
      },
      "source": [
        "## Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zY9GRd_Qxehl",
        "outputId": "4f637a7e-fd2c-41ce-f4e7-f16685796b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "39/39 [==============================] - 474s 12s/step - loss: 0.9402 - accuracy: 0.6062 - val_loss: 3.4018 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "39/39 [==============================] - 461s 12s/step - loss: 0.6531 - accuracy: 0.6802 - val_loss: 4.5987 - val_accuracy: 0.0357\n",
            "Epoch 3/10\n",
            "39/39 [==============================] - 472s 12s/step - loss: 0.6308 - accuracy: 0.6737 - val_loss: 5.1917 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 472s 12s/step - loss: 0.6308 - accuracy: 0.6737 - val_loss: 5.1917 - val_accuracy: 0.0357\n",
            "Epoch 4/10\n",
            "Epoch 4/10\n",
            "39/39 [==============================] - 461s 12s/step - loss: 0.6027 - accuracy: 0.6835 - val_loss: 5.6019 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 461s 12s/step - loss: 0.6027 - accuracy: 0.6835 - val_loss: 5.6019 - val_accuracy: 0.0357\n",
            "Epoch 5/10\n",
            "Epoch 5/10\n",
            "39/39 [==============================] - 475s 12s/step - loss: 0.5960 - accuracy: 0.7055 - val_loss: 5.9079 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 475s 12s/step - loss: 0.5960 - accuracy: 0.7055 - val_loss: 5.9079 - val_accuracy: 0.0357\n",
            "Epoch 6/10\n",
            "Epoch 6/10\n",
            "39/39 [==============================] - 457s 12s/step - loss: 0.5584 - accuracy: 0.7136 - val_loss: 6.1209 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 457s 12s/step - loss: 0.5584 - accuracy: 0.7136 - val_loss: 6.1209 - val_accuracy: 0.0357\n",
            "Epoch 7/10\n",
            "Epoch 7/10\n",
            "39/39 [==============================] - 465s 12s/step - loss: 0.5450 - accuracy: 0.7225 - val_loss: 6.4064 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 465s 12s/step - loss: 0.5450 - accuracy: 0.7225 - val_loss: 6.4064 - val_accuracy: 0.0357\n",
            "Epoch 8/10\n",
            "Epoch 8/10\n",
            "39/39 [==============================] - 440s 11s/step - loss: 0.5424 - accuracy: 0.7315 - val_loss: 6.3730 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 440s 11s/step - loss: 0.5424 - accuracy: 0.7315 - val_loss: 6.3730 - val_accuracy: 0.0357\n",
            "Epoch 9/10\n",
            "Epoch 9/10\n",
            "39/39 [==============================] - 447s 11s/step - loss: 0.5183 - accuracy: 0.7429 - val_loss: 6.6711 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 447s 11s/step - loss: 0.5183 - accuracy: 0.7429 - val_loss: 6.6711 - val_accuracy: 0.0357\n",
            "Epoch 10/10\n",
            "Epoch 10/10\n",
            "39/39 [==============================] - 451s 12s/step - loss: 0.4859 - accuracy: 0.7722 - val_loss: 6.4093 - val_accuracy: 0.0357\n",
            "39/39 [==============================] - 451s 12s/step - loss: 0.4859 - accuracy: 0.7722 - val_loss: 6.4093 - val_accuracy: 0.0357\n",
            "61/61 [==============================] - 740s 12s/step - loss: 2.1264 - accuracy: 0.3384\n",
            "61/61 [==============================] - 740s 12s/step - loss: 2.1264 - accuracy: 0.3384\n",
            "Test accuracy: 33.84%\n",
            "Test accuracy: 33.84%\n"
          ]
        }
      ],
      "source": [
        "conv_mixer_model = get_conv_mixer_256_8()\n",
        "history, conv_mixer_model = run_experiment(conv_mixer_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "convmixer",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}