{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVFWCJvo5lPU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP IMAGE CLASSFICATION\n"
      ],
      "metadata": {
        "id": "K0EHcPVyC43O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "dVskHhWx57DW",
        "outputId": "09ea428f-4c0a-41d1-f6ed-76963226105b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.1)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ipj3_rNe5lPV",
        "outputId": "fc85da7d-c557-4dff-9d72-4e1b3fbf84eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBIWdzZq6Sl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6KVYnork6TRF",
        "outputId": "3bd75822-c72c-4bf5-9146-f5d5a420eac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E26ljS5e5lPW"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "PxjI_d3I5lPX",
        "outputId": "79878b74-df08-4435-8246-ca08d2292b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6400 images belonging to 4 classes.\n",
            "Found 6400 images belonging to 4 classes.\n",
            "Training data shape: (128, 128, 3)\n",
            "Training labels shape: (6400,)\n",
            "Testing data shape: (128, 128, 3)\n",
            "Testing labels shape: (6400,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "# Define the paths to the training and testing data folders\n",
        "train_data_path = \"/content/drive/MyDrive/Azymer/AlzimerSVMModel/PreprocessSeg\"\n",
        "test_data_path = \"/content/drive/MyDrive/Azymer/AlzimerSVMModel/PreprocessSeg\"\n",
        "\n",
        "# Use TensorFlow's image data generator to load the images and labels from the folders\n",
        "train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "test_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_data_generator.flow_from_directory(directory=train_data_path,\n",
        "                                                      target_size=(128, 128),\n",
        "                                                      batch_size=32,\n",
        "                                                      class_mode='categorical',\n",
        "                                                      shuffle=True)\n",
        "\n",
        "test_data = test_data_generator.flow_from_directory(directory=test_data_path,\n",
        "                                                    target_size=(128,128),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=False)\n",
        "\n",
        "# Print the shapes of the data and labels\n",
        "print(\"Training data shape:\", train_data.image_shape)\n",
        "print(\"Training labels shape:\", train_data.classes.shape)\n",
        "print(\"Testing data shape:\", test_data.image_shape)\n",
        "print(\"Testing labels shape:\", test_data.classes.shape)\n",
        "\n",
        "# Assign the data and labels to variables x_train, y_train, x_test, and y_test\n",
        "x_train, y_train = train_data.next()\n",
        "x_test, y_test = test_data.next()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_split = 0.1\n",
        "\n",
        "val_indices = int(len(x_train) * val_split)\n",
        "new_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:]\n",
        "x_val, y_val = x_train[:val_indices], y_train[:val_indices]\n",
        "\n",
        "print(f\"Training data samples: {len(new_x_train)}\")\n",
        "print(f\"Validation data samples: {len(x_val)}\")\n",
        "print(f\"Test data samples: {len(x_test)}\")"
      ],
      "metadata": {
        "id": "1ulAXlQ38DVl",
        "outputId": "e8b64089-6e32-415b-ff43-f29affce1106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data samples: 29\n",
            "Validation data samples: 3\n",
            "Test data samples: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assume y_train and y_test have a shape of [?, 4]\n",
        "# Convert them to integer labels\n",
        "y_train_int = np.argmax(y_train, axis=1)\n",
        "y_test_int = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Now you can convert them to one-hot encoded vectors with num_classes=4\n",
        "y_train = tf.keras.utils.to_categorical(y_train_int, num_classes=4)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_int, num_classes=4)\n",
        "\n",
        "y_train = y_train.squeeze()\n",
        "y_test = y_test.squeeze()"
      ],
      "metadata": {
        "id": "-UR_6Cmu7T8v"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9CEP-NO5lPX"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "lM5FkE7w5lPY",
        "outputId": "b74d1308-02d2-4420-abd3-cffbd5567e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 8 X 8 = 64 \n",
            "Patches per image: 64\n",
            "Elements per patch (3 channels): 192\n"
          ]
        }
      ],
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "image_size = 64  # We'll resize input images to this size.\n",
        "patch_size = 8  # Size of the patches to be extracted from the input images.\n",
        "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
        "embedding_dim = 256  # Number of hidden units.\n",
        "num_blocks = 4  # Number of blocks.\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTfrs-LH5lPY"
      },
      "source": [
        "## Build a classification model\n",
        "\n",
        "We implement a method that builds a classifier given the processing blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "RaWWPe7A5lPZ"
      },
      "outputs": [],
      "source": [
        "input_shape=(128,128,3)\n",
        "num_classes=4\n",
        "def build_classifier(blocks, positional_encoding=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size, num_patches)(augmented)\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "    if positional_encoding:\n",
        "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "        position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=embedding_dim\n",
        "        )(positions)\n",
        "        x = x + position_embedding\n",
        "    # Process x using the module blocks.\n",
        "    x = blocks(x)\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes)(representation)\n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JH8vAoYfAsKW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2RYX_dD5lPa"
      },
      "source": [
        "## Define an experiment\n",
        "\n",
        "We implement a utility function to compile, train, and evaluate a given model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Y5SUCERX5lPa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    # Create Adam optimizer with weight decay.\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay,\n",
        "    )\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\", # use categorical crossentropy loss\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    # Create a learning rate scheduler callback.\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        new_x_train,\n",
        "        new_y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=50,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on the test set.\n",
        "    y_pred = model.predict(x_test)\n",
        "    accuracy = np.mean(np.argmax(y_test, axis=1) == np.argmax(y_pred, axis=1))\n",
        "    top_5_accuracy = np.mean(\n",
        "       [np.argmax(y_test[i]) in np.argsort(y_pred[i])[::-1][:5] for i in range(len(y_test))]\n",
        "    )\n",
        "\n",
        "# Print the test accuracies.\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72688F8C5lPb"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Bpx38c125lPc"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5N-hpI5lPc"
      },
      "source": [
        "## Implement patch extraction as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "cjfBA3TD5lPd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ruVUeDf5lPd"
      },
      "source": [
        "## The MLP-Mixer model\n",
        "\n",
        "The MLP-Mixer is an architecture based exclusively on\n",
        "multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n",
        "\n",
        "1. One applied independently to image patches, which mixes the per-location features.\n",
        "2. The other applied across patches (along channels), which mixes spatial information.\n",
        "\n",
        "This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n",
        "such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n",
        "instead of batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la14OFzD5lPd"
      },
      "source": [
        "### Implement the MLP-Mixer module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "mPBa4Efm5lPe"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLPMixerLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.mlp1 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=num_patches),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.mlp2 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=num_patches),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize(inputs)\n",
        "        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "        x_channels = tf.linalg.matrix_transpose(x)\n",
        "        # Apply mlp1 on each channel independently.\n",
        "        mlp1_outputs = self.mlp1(x_channels)\n",
        "        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "        # Add skip connection.\n",
        "        x = mlp1_outputs + inputs\n",
        "        # Apply layer normalization.\n",
        "        x_patches = self.normalize(x)\n",
        "        # Apply mlp2 on each patch independtenly.\n",
        "        mlp2_outputs = self.mlp2(x_patches)\n",
        "        # Add skip connection.\n",
        "        x = x + mlp2_outputs\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9gBDpmi5lPf"
      },
      "source": [
        "### Build, train, and evaluate the MLP-Mixer model\n",
        "\n",
        "Note that training the model with the current settings on a V100 GPUs\n",
        "takes around 8 seconds per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ZShnrGsW5lPf",
        "outputId": "6128685e-5ede-4e87-9058-9885188f35c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 7s 7s/step - loss: 8.9562 - accuracy: 0.5000 - val_loss: 10.8768 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 10.2069 - accuracy: 0.6154 - val_loss: 10.8047 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 10.0641 - accuracy: 0.6154 - val_loss: 10.7454 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 9.9188 - accuracy: 0.6154 - val_loss: 10.7454 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 8.0590 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 4.9594 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 4.9594 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0050\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0025\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0025\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0025\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0025\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 6.1993 - accuracy: 0.6154 - val_loss: 5.3727 - val_accuracy: 0.6667 - lr: 0.0025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe8b7c63e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Test accuracy: 0.0%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "mlpmixer_blocks = keras.Sequential(\n",
        "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.005\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n",
        "history = run_experiment(mlpmixer_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWfcGQ285lPf"
      },
      "source": [
        "The MLP-Mixer model tends to have much less number of parameters compared\n",
        "to convolutional and transformer-based models, which leads to less training and\n",
        "serving computational cost.\n",
        "\n",
        "As mentioned in the [MLP-Mixer](https://arxiv.org/abs/2105.01601) paper,\n",
        "when pre-trained on large datasets, or with modern regularization schemes,\n",
        "the MLP-Mixer attains competitive scores to state-of-the-art models.\n",
        "You can obtain better results by increasing the embedding dimensions,\n",
        "increasing the number of mixer blocks, and training the model for longer.\n",
        "You may also try to increase the size of the input images and use different patch sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVv7LpGk5lPg"
      },
      "source": [
        "## The FNet model\n",
        "\n",
        "The FNet uses a similar block to the Transformer block. However, FNet replaces the self-attention layer\n",
        "in the Transformer block with a parameter-free 2D Fourier transformation layer:\n",
        "\n",
        "1. One 1D Fourier Transform is applied along the patches.\n",
        "2. One 1D Fourier Transform is applied along the channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF156a4U5lPg"
      },
      "source": [
        "### Implement the FNet module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "ErblYL0e5lPh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FNetLayer(layers.Layer):\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim),\n",
        "                tfa.layers.GELU(),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Apply fourier transformations.\n",
        "        x = tf.cast(\n",
        "            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n",
        "            dtype=tf.dtypes.float32,\n",
        "        )\n",
        "        # Add skip connection.\n",
        "        x = x + inputs\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(x)\n",
        "        # Apply Feedfowrad network.\n",
        "        x_ffn = self.ffn(x)\n",
        "        # Add skip connection.\n",
        "        x = x + x_ffn\n",
        "        # Apply layer normalization.\n",
        "        return self.normalize2(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of8zv1qm5lPh"
      },
      "source": [
        "### Build, train, and evaluate the FNet model\n",
        "\n",
        "Note that training the model with the current settings on a V100 GPUs\n",
        "takes around 8 seconds per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "1ZWha9G05lPi",
        "outputId": "309a0d38-f4ab-4d2d-e495-3aa89f8738ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r1/1 [==============================] - ETA: 0s - loss: 3.9614 - accuracy: 0.0769"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 4s 4s/step - loss: 3.9614 - accuracy: 0.0769 - val_loss: 16.1181 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 11.8553 - accuracy: 0.0000e+00 - val_loss: 16.1181 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 10.5416 - accuracy: 0.0000e+00 - val_loss: 5.3727 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 6.8192 - accuracy: 0.0000e+00 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 5.5793 - accuracy: 0.0000e+00 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 3.7196 - accuracy: 0.0000e+00 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 3.7196 - accuracy: 0.0000e+00 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 3.7196 - accuracy: 0.0000e+00 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 3.7196 - accuracy: 0.0000e+00 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 3.7196 - accuracy: 0.0769 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 5.0000e-04\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 3.7196 - accuracy: 0.0385 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 5.0000e-04\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 3.7196 - accuracy: 0.0769 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 5.0000e-04\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 4.3395 - accuracy: 0.0385 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 5.0000e-04\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 3.7196 - accuracy: 0.1923 - val_loss: 1.1921e-07 - val_accuracy: 0.0000e+00 - lr: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 484ms/step\n",
            "Test accuracy: 0.0%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "fnet_blocks = keras.Sequential(\n",
        "    [FNetLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n",
        ")\n",
        "learning_rate = 0.001\n",
        "fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\n",
        "history = run_experiment(fnet_classifier)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mlp_image_classification",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}