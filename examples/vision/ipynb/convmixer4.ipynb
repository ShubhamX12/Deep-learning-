{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Convmixer Image Classfication\n"
      ],
      "metadata": {
        "id": "UTXuzSKBDvAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U -q tensorflow-addons"
      ],
      "metadata": {
        "id": "BTo-5q1eDsY0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uME3dCbWxed0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W-jSeUEDxed5",
        "outputId": "d88e6517-de75-47df-b3a0-a2d5843cf163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N6cx2lbHxeew"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GL6njKb2zo90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Bmx-0GxSzqNA",
        "outputId": "9f64dcee-2c92-43c2-fd75-9bfed84e423f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/PreprocessSeg/\"\n",
        "\n",
        "# Initialize empty lists to store the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Loop through all the subfolders and their files\n",
        "for subdir, _, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        # Load the image using OpenCV\n",
        "        image = cv2.imread(os.path.join(subdir, file))\n",
        "        # Resize the image to the desired size\n",
        "        image = cv2.resize(image, (64, 64))\n",
        "        # Normalize the image pixel values to be between 0 and 1\n",
        "        image = image / 255.0\n",
        "        # Append the image to the list of images\n",
        "        images.append(image)\n",
        "        # Extract the label from the folder name\n",
        "        label = os.path.basename(subdir)\n",
        "        # Append the label to the list of labels\n",
        "        labels.append(label)\n",
        "\n",
        "# Convert the lists of images and labels to numpy arrays\n",
        "images = np.array(images)\n",
        "num_classes = 4\n",
        "label_map = {'Mild_Demented': 0, 'Moderate_Demented': 1, 'Non_Demented': 2, 'Very_Mild_Demented': 3}\n",
        "labels_int = np.array([label_map[label] for label in labels])\n",
        "labels_one_hot = np.eye(num_classes)[labels_int]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "val_split = 0.1\n",
        "test_split = 0.1\n",
        "num_val = int(len(images) * val_split)\n",
        "num_test = int(len(images) * test_split)\n",
        "num_train = len(images) - num_val - num_test\n",
        "\n",
        "x_train, y_train = images[:num_train], labels_one_hot[:num_train]\n",
        "x_val, y_val = images[num_train:num_train+num_val], labels_one_hot[num_train:num_train+num_val]\n",
        "x_test, y_test = images[num_train+num_val:], labels_one_hot[num_train+num_val:]\n",
        "\n",
        "len(x_train)"
      ],
      "metadata": {
        "id": "-GhlxH4Hzk-T",
        "outputId": "01251a0e-961c-474c-ffe0-fe8c8ae1213b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1537"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ReP2ciSckvAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SQdtTY8xegV",
        "outputId": "2dc76115-db10-4d70-8fa4-79b60a4815d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data samples: 29\n",
            "Validation data samples: 3\n",
            "Test data samples: 32\n"
          ]
        }
      ],
      "source": [
        "#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "val_split = 0.1\n",
        "\n",
        "val_indices = int(len(x_train) * val_split)\n",
        "new_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:]\n",
        "x_val, y_val = x_train[:val_indices], y_train[:val_indices]\n",
        "\n",
        "print(f\"Training data samples: {len(new_x_train)}\")\n",
        "print(f\"Validation data samples: {len(x_val)}\")\n",
        "print(f\"Test data samples: {len(x_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assume y_train and y_test have a shape of [?, 4]\n",
        "# Convert them to integer labels\n",
        "y_train_int = np.argmax(y_train, axis=1)\n",
        "y_test_int = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Now you can convert them to one-hot encoded vectors with num_classes=4\n",
        "y_train = tf.keras.utils.to_categorical(y_train_int, num_classes=4)\n",
        "y_test = tf.keras.utils.to_categorical(y_test_int, num_classes=4)\n",
        "\n",
        "y_train = y_train.squeeze()\n",
        "y_test = y_test.squeeze()\n"
      ],
      "metadata": {
        "id": "W6Br9_MSkIOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdwVUtpXxegn"
      },
      "source": [
        "## ConvMixer utilities\n",
        "\n",
        "The following figure (taken from the original paper) depicts the ConvMixer model:\n",
        "\n",
        "![](https://i.imgur.com/yF8actg.png)\n",
        "\n",
        "ConvMixer is very similar to the MLP-Mixer, model with the following key\n",
        "differences:\n",
        "\n",
        "* Instead of using fully-connected layers, it uses standard convolution layers.\n",
        "* Instead of LayerNorm (which is typical for ViTs and MLP-Mixers), it uses BatchNorm.\n",
        "\n",
        "Two types of convolution layers are used in ConvMixer. **(1)**: Depthwise convolutions,\n",
        "for mixing spatial locations of the images, **(2)**: Pointwise convolutions (which follow\n",
        "the depthwise convolutions), for mixing channel-wise information across the patches.\n",
        "Another keypoint is the use of *larger kernel sizes* to allow a larger receptive field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YPE2IxmbxehD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def activation_block(x):\n",
        "    x = layers.Activation(\"gelu\")(x)\n",
        "    return layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int):\n",
        "    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "    return activation_block(x)\n",
        "\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n",
        "    x = layers.Add()([activation_block(x), x0])  # Residual.\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = activation_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_conv_mixer_256_8(\n",
        "    image_size=64, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=4\n",
        "):\n",
        "    \"\"\"ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.\n",
        "    The hyperparameter values are taken from the paper.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size)\n",
        "\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size)\n",
        "\n",
        "    # Classification block.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKISV-yrxehS"
      },
      "source": [
        "## Model training and evaluation utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gq1Box8BxehT"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\", \n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train, \n",
        "        y_train,  \n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model.evaluate(x_test,y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu3Lk-zWxehW"
      },
      "source": [
        "## Train and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY9GRd_Qxehl",
        "outputId": "0340e9e1-5b9b-40ef-a042-b720de4ab060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "39/39 [==============================] - 391s 10s/step - loss: 1.0910 - accuracy: 0.6428 - val_loss: 2.2938 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "39/39 [==============================] - 357s 9s/step - loss: 0.7393 - accuracy: 0.6729 - val_loss: 3.8225 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "35/39 [=========================>....] - ETA: 34s - loss: 0.6000 - accuracy: 0.7054"
          ]
        }
      ],
      "source": [
        "conv_mixer_model = get_conv_mixer_256_8()\n",
        "history, conv_mixer_model = run_experiment(conv_mixer_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "convmixer",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}